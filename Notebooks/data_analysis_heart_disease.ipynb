{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d63f4e",
   "metadata": {},
   "source": [
    "# Análisis de Datos: Heart Disease (Enfermedad Cardiaca)\n",
    "\n",
    "Notebook preparado para Google Colab — actividad de ML.\n",
    "\n",
    "**Autor:** Adrian David Gonzalez Romero\n",
    "\n",
    "**Objetivo:** Realizar EDA, preprocesamiento y comparar modelos de clasificación\n",
    "para predecir la presencia de enfermedad cardíaca.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbde185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración inicial ---\n",
    "# Ejecuta en Google Colab. Si no estás en Colab, asegúrate de tener las librerías instaladas.\n",
    "!pip install -q xgboost shap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style='whitegrid')\n",
    "print('Librerías cargadas correctamente.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f385b",
   "metadata": {},
   "source": [
    "## 1) Carga del dataset\n",
    "\n",
    "Usaremos la versión procesada del dataset de Heart Disease (Cleveland) desde UCI. El archivo será descargado directamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34327b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar y cargar datos desde UCI (procesado - Cleveland)\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n",
    "cols = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','target']\n",
    "df = pd.read_csv(url, header=None, names=cols, na_values='?')\n",
    "print('Datos cargados. Shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06971eb",
   "metadata": {},
   "source": [
    "## 2) Exploración de datos (EDA)\n",
    "Mostraremos estadísticas descriptivas, valores faltantes y algunas visualizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general\n",
    "df.info()\n",
    "print('\\nDescripción estadística:')\n",
    "display(df.describe())\n",
    "\n",
    "# Valores faltantes\n",
    "print('\\nValores faltantes por columna:')\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Distribución de la variable objetivo\n",
    "# En este dataset 'target' >0 indica presencia de enfermedad (transformaremos a 0/1)\n",
    "display(df['target'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=(df['target']>0).astype(int))\n",
    "plt.title('Distribución: presencia de enfermedad cardíaca (0=no, 1=sí)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26510d",
   "metadata": {},
   "source": [
    "## 3) Preprocesamiento\n",
    "Limpieza, imputación, codificación (si aplica) y escalado.\n",
    "Transformaremos `target` a 0/1 donde 1 indica presencia de enfermedad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1faeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar target a 0/1\n",
    "df['target'] = (df['target'] > 0).astype(int)\n",
    "\n",
    "# Imputar columnas numéricas (ca, thal tienen '?', ya convertidos a NaN)\n",
    "num_cols = df.select_dtypes(include=['float64','int64']).columns.tolist()\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[num_cols] = imputer.fit_transform(df[num_cols])\n",
    "\n",
    "# Revisar datos categóricos que puedan necesitar one-hot (cp, restecg, slope, thal)\n",
    "df[['cp','restecg','slope','thal','ca']] = df[['cp','restecg','slope','thal','ca']].astype(int)\n",
    "\n",
    "# One-hot para 'cp' y 'thal' y 'slope' si se desea:\n",
    "df = pd.get_dummies(df, columns=['cp','thal','slope','restecg'], drop_first=True)\n",
    "\n",
    "# Separar X y y\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print('Shape X:', X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecff72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Conjuntos preparados: X_train', X_train.shape, 'X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47601e",
   "metadata": {},
   "source": [
    "## 4) Modelado\n",
    "Entrenaremos: Regresión Logística, Random Forest y XGBoost. Se mostrarán métricas y matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Regresión Logística (baseline)\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "print('Logistic Regression')\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print('Accuracy:', accuracy_score(y_test,y_pred_lr))\n",
    "print('ROC AUC:', roc_auc_score(y_test, lr.predict_proba(X_test_scaled)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2959f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print('Random Forest')\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print('Accuracy:', accuracy_score(y_test,y_pred_rf))\n",
    "print('ROC AUC:', roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 XGBoost\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "print('XGBoost')\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print('Accuracy:', accuracy_score(y_test,y_pred_xgb))\n",
    "print('ROC AUC:', roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7908251",
   "metadata": {},
   "source": [
    "## 5) Importancia de características y visualizaciones finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancia de features con RandomForest\n",
    "importances = rf.feature_importances_\n",
    "feat_names = X.columns\n",
    "feat_imp = pd.Series(importances, index=feat_names).sort_values(ascending=False)\n",
    "display(feat_imp.head(15))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=feat_imp.head(15).values, y=feat_imp.head(15).index)\n",
    "plt.title('Top 15 - Importancia de features (RandomForest)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f01e02",
   "metadata": {},
   "source": [
    "## 6) Conclusiones y recomendaciones\n",
    "\n",
    "- Resumen de hallazgos.\n",
    "- Limitaciones del dataset y del análisis.\n",
    "- Recomendaciones para mejorar el modelo (más datos, validación cruzada, ajuste de hiperparámetros, uso de técnicas de balanceo si fuera necesario).\n",
    "\n",
    "Puedes extender este notebook: ajuste de hiperparámetros con GridSearchCV, validación cruzada, uso de SHAP para interpretabilidad, pipelines de sklearn, etc."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
